{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/saiprakashgorti/Illinois-CS-441/blob/main/CS441_FA24_HW5_Starter.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gd7scc-8QJvE"
      },
      "source": [
        "## CS441: Applied ML - HW 5"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part 1: Applications of AI\n",
        "Nothing to code for this part."
      ],
      "metadata": {
        "id": "j3GV4iHJa1bU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part 2: Fine-Tune for Pets Image Classification\n"
      ],
      "metadata": {
        "id": "aEqhjXvX2SEz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.1 Prepare Data"
      ],
      "metadata": {
        "id": "8VnGuQjt2yKG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim.lr_scheduler as lrs\n",
        "from torch.utils.data import DataLoader\n",
        "import torchvision\n",
        "from torchvision import datasets\n",
        "from torchvision import transforms\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "\n",
        "import os\n",
        "from pathlib import Path\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "cdImCd6foYCc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Mount and define data dir\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "datadir = \"/content/\"\n",
        "save_dir = \"/content/drive/My Drive/CS441/24SP/hw5\"  # change to your directory"
      ],
      "metadata": {
        "id": "qHqtzjWwpAG2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_pet_dataset(train_transform = None, test_transform = None):\n",
        "    OxfordIIITPet = datasets.OxfordIIITPet\n",
        "    if os.path.isdir(datadir+ \"oxford-iiit-pet\"):\n",
        "      do_download = False\n",
        "    else:\n",
        "      do_download = True\n",
        "    training_set = OxfordIIITPet(root = datadir,\n",
        "                             split = 'trainval',\n",
        "                             transform = train_transform,\n",
        "                             download = do_download)\n",
        "\n",
        "    test_set = OxfordIIITPet(root = datadir,\n",
        "                           split = 'test',\n",
        "                           transform = test_transform,\n",
        "                           download = do_download)\n",
        "    return training_set, test_set\n"
      ],
      "metadata": {
        "id": "4M4q5zIHoYKQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_set, test_set = load_pet_dataset()\n",
        "\n",
        "# Display a sample in OxfordIIIPet dataset\n",
        "sample_idx = 0 # Choose an image index that you want to display\n",
        "print(\"Label:\", train_set.classes[train_set[sample_idx][1]])\n",
        "train_set[sample_idx][0]"
      ],
      "metadata": {
        "id": "C4TLZkEnLW1w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.2 Data Preprocess"
      ],
      "metadata": {
        "id": "8WxSQnR3oBI9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision import transforms\n",
        "from torch.utils.data import DataLoader"
      ],
      "metadata": {
        "id": "pVS7ycpYMXai"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Feel free to add augmentation choices\n",
        "\n",
        "# Apply data transformation/augmentation\n",
        "train_transform = transforms.Compose([\n",
        "            transforms.Resize(224),\n",
        "            transforms.CenterCrop(224),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                                 std= [0.229, 0.224, 0.225]),\n",
        "        ])\n",
        "\n",
        "test_transform = transforms.Compose([\n",
        "            transforms.Resize(224),  # resize to 224x224 because that's the size of ImageNet images\n",
        "            transforms.CenterCrop(224),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                                 std= [0.229, 0.224, 0.225]),\n",
        "        ])"
      ],
      "metadata": {
        "id": "2ycl4Q1uMBFI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Feel free to change batch size\n",
        "train_set, test_set = load_pet_dataset(train_transform, test_transform)\n",
        "train_loader = DataLoader(dataset=train_set,\n",
        "                          batch_size=64,\n",
        "                          shuffle=True,\n",
        "                          num_workers=2)\n",
        "\n",
        "test_loader = DataLoader(dataset=test_set,\n",
        "                          batch_size=64,\n",
        "                          shuffle=False,\n",
        "                          num_workers=2)\n"
      ],
      "metadata": {
        "id": "6fEFI_CVMA9m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.3 Helper Functions\n",
        "\n"
      ],
      "metadata": {
        "id": "i7wFK09AoEiV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Display the number of parameters and model structure\n",
        "def display_model(model):\n",
        "  # Check number of parameters\n",
        "  summary_dict = {}\n",
        "  num_params = 0\n",
        "  summary_str = ['='*80]\n",
        "\n",
        "  for module_name, module in model.named_children():\n",
        "      summary_count = 0\n",
        "      for name, param in module.named_parameters():\n",
        "          if(param.requires_grad):\n",
        "              summary_count += param.numel()\n",
        "              num_params += param.numel()\n",
        "      summary_dict[module_name] = [summary_count]\n",
        "      summary_str+= [f'- {module_name: <40} : {str(summary_count):^34s}']\n",
        "\n",
        "  summary_dict['total'] = [num_params]\n",
        "\n",
        "  # print summary string\n",
        "  summary_str += ['='*80]\n",
        "  summary_str += ['--' +  f'{\"Total\":<40} : {str(num_params) + \" params\":^34s}' +'--']\n",
        "  print('\\n'.join(summary_str))\n",
        "\n",
        "  # print model structure\n",
        "  print(model)"
      ],
      "metadata": {
        "id": "kHgfXgw1HaZd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot loss or accuracy\n",
        "def plot_losses(train, val, test_frequency, num_epochs):\n",
        "    plt.plot(train, label=\"train\")\n",
        "    indices = [i for i in range(num_epochs) if ((i+1)%test_frequency == 0 or i ==0 or i == 1)]\n",
        "    plt.plot(indices, val, label=\"val\")\n",
        "    plt.title(\"Loss Plot\")\n",
        "    plt.ylabel(\"Loss\")\n",
        "    plt.xlabel(\"Epoch\")\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "def plot_accuracy(train, val, test_frequency, num_epochs):\n",
        "    indices = [i for i in range(num_epochs) if ((i+1)%test_frequency == 0 or i ==0 or i == 1)]\n",
        "    plt.plot(indices, train, label=\"train\")\n",
        "    plt.plot(indices, val, label=\"val\")\n",
        "    plt.title(\"Accuracy Plot\")\n",
        "    plt.ylabel(\"Accuracy\")\n",
        "    plt.xlabel(\"Epoch\")\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "def save_checkpoint(save_dir, model, save_name = 'best_model.pth'):\n",
        "    save_path = os.path.join(save_dir, save_name)\n",
        "    torch.save(model.state_dict(), save_path)\n",
        "\n",
        "def load_model(model, save_dir, save_name = 'best_model.pth'):\n",
        "    save_path = os.path.join(save_dir, save_name)\n",
        "    model.load_state_dict(torch.load(save_path))\n",
        "    return model"
      ],
      "metadata": {
        "id": "-poihz_hoN0N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.4 YOUR TASK: Fine-Tune Pre-trained Network on Pets\n",
        "\n",
        "A convolutional neural network (CNN) is a type of deep learning artificial neural network commonly used for image processing tasks. In this section, you will load a pretrained network, finetune it for a new task, and evaluate it to achieve a good image classification accuracy on the Oxford-IIIT Pet dataset. The task is to classify the breed of cat or dog.\n",
        "\n",
        "To save you time and focus on the interesting parts, we’ve provided helper functions and much of the code structure. Read it carefully and complete/uncomment it as you go. Your key steps are:\n",
        "\n",
        "1. Load a pretrained ResNet34 and replace the last layer of the network so that the output dimension matches the number of Pet classes. Use the printout of network structure and parameters to answer questions in your report.\n",
        "\n",
        "2. Set the learning rate and learning scheduler, train, and evaluate.  You may need to modify the training settings to get a better performance. You can consider the data augmentation, learning rate, learning rate scheduler, and batch size. Attach the accuracy and loss plots for training_set and test_set, and report your best testing accuracy. You should be able to reach an accuracy of at least 90% within 10 epochs.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "mDkQzodjoIBf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# set device, using GPU 'cuda' will be faster\n",
        "device = 'cuda'"
      ],
      "metadata": {
        "id": "6t4jfUX_etq7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(train_loader, model, criterion, optimizer):\n",
        "    \"\"\"\n",
        "    Train network\n",
        "    :param train_loader: training dataloader\n",
        "    :param model: model to be trained\n",
        "    :param criterion: criterion used to calculate loss (should be CrossEntropyLoss from torch.nn)\n",
        "    :param optimizer: optimizer for model's params (Adams or SGD)\n",
        "    :return: mean training loss\n",
        "    \"\"\"\n",
        "    model.train()\n",
        "    loss_ = 0.0\n",
        "    losses = []\n",
        "\n",
        "    # TO DO: read this documentation  https://pypi.org/project/tqdm/\n",
        "    it_train = tqdm(enumerate(train_loader), total=len(train_loader), desc=\"Training ...\", position = 0) # progress bar\n",
        "    for i, (images, labels) in it_train:\n",
        "\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "        # zero the gradient\n",
        "        # TO DO\n",
        "\n",
        "        # predict labels\n",
        "        # TO DO\n",
        "\n",
        "        # compute loss\n",
        "        # TO DO\n",
        "\n",
        "        # compute gradients\n",
        "        # TO DO\n",
        "\n",
        "        # update weights\n",
        "        # TO DO\n",
        "\n",
        "        # keep track of losses\n",
        "        losses.append(loss)\n",
        "\n",
        "        # set text to display\n",
        "        it_train.set_description(f'loss: {loss:.3f}')\n",
        "\n",
        "    return torch.stack(losses).mean().item()\n",
        "\n",
        "def test(test_loader, model, criterion):\n",
        "    \"\"\"\n",
        "    Test network.\n",
        "    :param test_loader: testing dataloader\n",
        "    :param model: model to be tested\n",
        "    :param criterion: criterion used to calculate loss (should be CrossEntropyLoss from torch.nn)\n",
        "    :return: mean_accuracy: mean accuracy of predicted labels\n",
        "             test_loss: mean test loss during testing\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    losses = []\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    # TO DO: read this documentation and then uncomment the line below; https://pypi.org/project/tqdm/\n",
        "    #it_test = tqdm(enumerate(test_loader), total=len(test_loader), desc=\"Validating ...\", position = 0)\n",
        "    for i, (images, labels) in it_test:\n",
        "\n",
        "      # TO DO: read/understand and then uncomment these lines\n",
        "\n",
        "      #images, labels = images.to(device), labels.to(device)\n",
        "      #with torch.no_grad():  # https://pytorch.org/docs/stable/generated/torch.no_grad.html\n",
        "      #  output = model(images) # do not compute gradient when performing prediction\n",
        "      #preds = torch.argmax(output, dim=-1)\n",
        "      #loss = criterion(output, labels)\n",
        "      #losses.append(loss.item())\n",
        "      #correct += (preds == labels).sum().item()\n",
        "      #total += len(labels)\n",
        "\n",
        "    mean_accuracy = correct / total\n",
        "    test_loss = np.mean(losses)\n",
        "    print('Mean Accuracy: {0:.4f}'.format(mean_accuracy))\n",
        "    print('Avg loss: {}'.format(test_loss))\n",
        "\n",
        "    return mean_accuracy, test_loss"
      ],
      "metadata": {
        "id": "1KxcnMexI22W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def initializeModel(num_target_classes = 37):\n",
        "  # loads a pre-trained ResNet-34 model\n",
        "  model = torch.hub.load('pytorch/vision:v0.10.0', 'resnet34', pretrained=True)\n",
        "\n",
        "  # TO DO: replace the last layer (classification head) with a new linear layer for Pets classification here\n",
        "\n",
        "  model = model.to(device)\n",
        "  display_model(model) # displays the model structure and parameter count\n",
        "\n",
        "  return model"
      ],
      "metadata": {
        "id": "YvjJaCVMe47r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training Settings. Feel free to change.\n",
        "\n",
        "model = initializeModel()\n",
        "\n",
        "num_epochs = 10\n",
        "test_interval = 1\n",
        "\n",
        "# TO DO: set initial learning rate\n",
        "learn_rate =\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learn_rate)\n",
        "\n",
        "# TO DO: define your learning rate scheduler, e.g. StepLR\n",
        "# https://pytorch.org/docs/stable/optim.html#module-torch.optim.lr_scheduler\n",
        "lr_scheduler =  torch.optim.lr_scheduler.XXX\n",
        "\n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "train_losses = []\n",
        "train_accuracy_list = []\n",
        "test_losses = []\n",
        "test_accuracy_list = []\n",
        "\n",
        "\n",
        "# Iterate over the DataLoader for training data\n",
        "for epoch in tqdm(range(num_epochs), total=num_epochs, desc=\"Training ...\", position=1):\n",
        "\n",
        "    # Train the network for one epoch\n",
        "    train_loss = train(train_loader, model, criterion, optimizer)\n",
        "\n",
        "    # TO DO: uncomment the line below. It should be called each epoch to apply the lr_scheduler\n",
        "    #lr_scheduler.step()\n",
        "\n",
        "    train_losses.append(train_loss)\n",
        "    print(f'Loss for Training on epoch {str(epoch)} is {str(train_loss)} \\n')\n",
        "\n",
        "    # Get the train accuracy and test loss/accuracy\n",
        "    if(epoch%test_interval==0 or epoch==1 or epoch==num_epochs-1):\n",
        "        print('Evaluating Network')\n",
        "\n",
        "        train_accuracy, _ = test(train_loader, model, criterion) # Get training accuracy\n",
        "        train_accuracy_list.append(train_accuracy)\n",
        "\n",
        "        print(f'Training accuracy on epoch {str(epoch)} is {str(train_accuracy)} \\n')\n",
        "\n",
        "        test_accuracy, test_loss = test(test_loader, model, criterion) # Get testing accuracy and error\n",
        "        test_losses.append(test_loss)\n",
        "        test_accuracy_list.append(test_accuracy)\n",
        "        print(f'Test (val) accuracy on epoch {str(epoch)} is {str(test_accuracy)} \\n')\n",
        "\n",
        "        # Checkpoints are used to save the model with best validation accuracy\n",
        "        if test_accuracy >= max(test_accuracy_list):\n",
        "          print(\"Saving Model\")\n",
        "          save_checkpoint(save_dir, model, save_name = 'best_model.pth') # Save model with best performance\n"
      ],
      "metadata": {
        "id": "raaHUabTckg7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot losses and accuracy\n",
        "\n",
        "plot_losses(train_losses, test_losses, test_interval, num_epochs)\n",
        "plot_accuracy(train_accuracy_list, test_accuracy_list, test_interval, num_epochs)"
      ],
      "metadata": {
        "id": "eTz1EeJncj-d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TO DO: initialize your trained model as you did before so that you can load the parameters into it\n",
        "\n",
        "model = initializeModel()\n",
        "\n",
        "load_model(model, save_dir) # Load the trained weights\n",
        "\n",
        "test_accuracy, test_loss= test(test_loader, model, criterion)\n",
        "print(f\"Validation accuracy is {str(test_accuracy)} \\n\")"
      ],
      "metadata": {
        "id": "kzIdvHhTJ1uz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part 3: CLIP: Contrastive Language-Image Pretraining\n",
        "Include all the code for Part 3 in this section"
      ],
      "metadata": {
        "id": "B3Bmk_x8P4uu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Prepare data"
      ],
      "metadata": {
        "id": "b6JyJbuCVBSA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[Here](https://drive.google.com/file/d/1zJ1KfymSfsbmD6QS-F0eUC8T1PkqW0_j/view?usp=sharing) is the json file you need for labels of flowers 102"
      ],
      "metadata": {
        "id": "85mSljTL-Q8W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import os\n",
        "import os.path as osp\n",
        "import numpy as np\n",
        "from google.colab import drive\n",
        "import torch\n",
        "from torchvision.datasets import Flowers102\n",
        "%matplotlib inline\n",
        "from matplotlib import pyplot as plt"
      ],
      "metadata": {
        "id": "C4nnOrdbVE_o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "drive.mount('/content/drive')\n",
        "#datadir = \"/content/drive/My Drive/CS441/24FA/hw5/\"  # if you copy the json to Google Drive, you'll just have to load the images once\n",
        "datadir = \".\"\n"
      ],
      "metadata": {
        "id": "UVFkDZ6eRNWj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_flower_data(img_transform=None):\n",
        "    if os.path.isdir(datadir+ \"flowers-102\"):\n",
        "      do_download = False\n",
        "    else:\n",
        "      do_download = True\n",
        "    train_set = Flowers102(root=datadir, split='train', transform=img_transform, download=do_download)\n",
        "    test_set = Flowers102(root=datadir, split='val', transform=img_transform, download=do_download)\n",
        "    classes = json.load(open(osp.join(datadir, \"flowers102_classes.json\")))\n",
        "\n",
        "    return train_set, test_set, classes"
      ],
      "metadata": {
        "id": "CpIrWhS8VM4B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# READ ME!  This takes some time (a few minutes), so if you are using Colabs and want to use GPU for speed,\n",
        "#           first set to use GPU: Edit->Notebook Settings->Hardware Accelerator=GPU, and restart instance\n",
        "\n",
        "# Data structure details\n",
        "#   flower_train[n][0] is the nth train image\n",
        "#   flower_train[n][1] is the nth train label\n",
        "#   flower_test[n][0] is the nth test image\n",
        "#   flower_test[n][1] is the nth test label\n",
        "#   flower_classes[k] is the name of the kth class\n",
        "flower_train, flower_test, flower_classes = load_flower_data()"
      ],
      "metadata": {
        "id": "rceL8iFHVPWx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(flower_train), len(flower_test)  # output should be (1020, 1020)"
      ],
      "metadata": {
        "id": "sEgaoLcxVS8z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Display a sample in Flowers 102 dataset\n",
        "sample_idx = 0 # Choose an image index that you want to display\n",
        "print(\"Label:\", flower_classes[flower_train[sample_idx][1]])\n",
        "flower_train[sample_idx][0]"
      ],
      "metadata": {
        "id": "HMQXeEFzVXrK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Prepare CLIP model"
      ],
      "metadata": {
        "id": "eiNItn6aVb4R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install git+https://github.com/openai/CLIP.git"
      ],
      "metadata": {
        "id": "XS2djYMFVgAu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import clip"
      ],
      "metadata": {
        "id": "-GYOR8A8VhPU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Sets device to \"cuda\" if a GPU is available\n",
        "device = \"cuda\" if torch.cuda.is_available() else 'cpu'\n",
        "print(device)\n",
        "# If this takes a really long time, stop and then restart the download\n",
        "clip_model, clip_preprocess = clip.load(\"ViT-B/32\", device=device)"
      ],
      "metadata": {
        "id": "nCREo-mfVk7c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### CLIP zero-shot prediction example"
      ],
      "metadata": {
        "id": "nqopdlSoVn-h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"The following is an example of using CLIP pre-trained model for zero-shot prediction task\"\"\"\n",
        "# Prepare the inputs\n",
        "n = 100  # image index to use\n",
        "image, class_id = flower_train[n]\n",
        "image_input = clip_preprocess(image).unsqueeze(0).to(device) # extract image and put in device memory\n",
        "text_inputs = torch.cat([clip.tokenize(f\"a photo of a {c}, a type of flower.\") for c in flower_classes]).to(device) # put text to match to image in device memory\n",
        "\n",
        "# Calculate features\n",
        "with torch.no_grad():\n",
        "    image_features = clip_model.encode_image(image_input) # compute image features with CLIP model\n",
        "    text_features = clip_model.encode_text(text_inputs) # compute text features with CLIP model\n",
        "image_features /= image_features.norm(dim=-1, keepdim=True) # unit-normalize image features\n",
        "text_features /= text_features.norm(dim=-1, keepdim=True) # unit-normalize text features\n",
        "\n",
        "# Pick the top 5 most similar labels for the image\n",
        "similarity = (100.0 * image_features @ text_features.T) # score is cosine similarity times 100\n",
        "p_class_given_image= similarity.softmax(dim=-1)  # P(y|x) is score through softmax\n",
        "values, indices = p_class_given_image[0].topk(5) # gets the top 5 labels\n",
        "\n",
        "# Print the probability of the top five labels\n",
        "print(\"Ground truth:\", flower_classes[class_id])\n",
        "print(\"\\nTop predictions:\\n\")\n",
        "for value, index in zip(values, indices):\n",
        "    print(f\"{flower_classes[index]:>16s}: {100 * value.item():.2f}%\")\n",
        "image"
      ],
      "metadata": {
        "id": "JoroMsvhVqWi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.1 YOUR TASK: Test CLIP zero-shot performance on Flowers 102\n",
        "\n",
        " Use pre-trained text and image representations to classify images. For zero-shot recognition, text features are computed from the CLIP model for phrases such as  “An image of [flower_name], a type of flower” for varying [flower_name] inserts. Then, image features are computed using the CLIP model for an image, and the cosine similarity between each text and image is computed. The label corresponding to the most similar text is assigned to the image. You'll get that working using a data loader, which enables faster batch processing; then, compute the accuracy over the test set.  You should see top-1 accuracy in the 60-70% range.\n",
        "\n",
        " For zero-shot, you do not use the training set at all.   You should only have to compute the text vectors once and re-use them for all test images.\n",
        "\n",
        " Basic steps:\n",
        "\n",
        " 1. Create the normalized CLIP text vectors for each class label.\n",
        " 2. For each batch:\n",
        "       * Create normalized CLIP image vectors\n",
        "       * Compute similarity between text and image vectors\n",
        "       * Get index of most likely class label and check whether it matches the ground truth\n",
        "       * Keep a count of number correct and number total\n",
        " 3. Return accuracy = # correct / # total  \n"
      ],
      "metadata": {
        "id": "-xsgj_yIWA7R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "from torch.utils.data import DataLoader"
      ],
      "metadata": {
        "id": "P-1MRojmV_KF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load flowers dataset again. This time, with clip_preprocess as transform (you don't have to call clip_preprocess again)\n",
        "flower_train_trans, flower_test_trans, flower_classes = load_flower_data(img_transform=clip_preprocess)"
      ],
      "metadata": {
        "id": "QVsfdPPzWKPR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def clip_zero_shot(data_set, classes):\n",
        "    data_loader = DataLoader(data_set, batch_size=32, shuffle=False)  # dataloader lets you process in batch which is way faster (when using GPU)\n",
        "\n",
        "    # TO DO: Needs code here\n",
        "\n",
        "    return accuracy"
      ],
      "metadata": {
        "id": "4C2Ut3sZWMZ3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "accuracy = clip_zero_shot(data_set=flower_test_trans, classes=flower_classes)\n",
        "print(f\"\\nAccuracy = {100*accuracy:.3f}%\")"
      ],
      "metadata": {
        "id": "5_L3ntLcWazE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.2 YOUR TASK: Test CLIP linear probe performance on Flowers 102\n",
        "\n",
        "We do not use text features for the linear probe method.  Train on the train set, and evaluate on the test set and report your performance. You can get top-1 accuracy in the 90-95% range.  If you’re getting in the 80’s, try both normalizing and not normalizing the features."
      ],
      "metadata": {
        "id": "hABMuuN6WwSa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression"
      ],
      "metadata": {
        "id": "pT4IF_4SWvj8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Returns image features and labels in numpy format.\n",
        "The labels should just be integers representing class index, not text vectors.\n",
        "\"\"\"\n",
        "def get_features(data_set):\n",
        "    # TO DO: Needs code here to extract features and labels\n"
      ],
      "metadata": {
        "id": "t9s_yFNEW0bj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate the image features\n",
        "train_features, train_labels = get_features(flower_train_trans)\n",
        "test_features, test_labels = get_features(flower_test_trans)\n",
        "\n",
        "# TO DO: Needs code here\n",
        "# Train logistic regression model with train_features, train_labels\n",
        "\n",
        "# Evaluate accuracy on test_features, test_labels\n",
        "\n",
        "print(f\"\\nAccuracy = {100*accuracy:.3f}%\")"
      ],
      "metadata": {
        "id": "yRFUbLtwXN05"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.3 YOUR TASK: Evaluate a nearest-neighbor classifier on CLIP features\n",
        "\n",
        "Extract features based on the pre-trained model (can be the same features as 3.5) and apply a nearest neighbor classifier.  You can use your own implementation of nearest neighbor or a library like sklearn or FAISS for this.  Try K={1, 3, 5, 7, 11, 21}.  If using sklearn, you can also experiment with 'uniform' and 'distance' weighting. Report performance for best K on the test set.  You can also experiment with using unnormalized or normalized features. You should see top-1 accuracy in the 80-90% range."
      ],
      "metadata": {
        "id": "NWI6aAs2Xypt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TO DO: code for KNN prediction and evaluation (may use sklearn.neighbors.NeighborsClassifier or FAISS or own implementation)\n",
        "print(f\"\\nAccuracy = {100*accuracy:.3f}%\")"
      ],
      "metadata": {
        "id": "EqAESETdYaW4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 4: Stretch Goals\n",
        "Include any new code needed for Part 4 here."
      ],
      "metadata": {
        "id": "3X3j_efPhh6e"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4.a Compare word tokenizers\n",
        "\n",
        "Train at least two 8K token word tokenizers (e.g. BPE, WordPiece, SentencePiece) on the WikiText-2, and compare their encodings. You can use existing libraries, such as those linked below to train and encode/decode. Report the encodings for “I am learning about word tokenizers. They are not very complicated, and they are a good way to convert natural text into tokens.”  E.g. “I am the fastest planet” may end up being tokenized as [I, _am, _the, _fast, est, _plan, et].  Also, report the tokenizations of an additional sentence of your choice that results in different encodings by the two models.\n",
        "\n",
        "https://github.com/huggingface/tokenizers\n"
      ],
      "metadata": {
        "id": "YVcNSHsl9SSu"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "VP9JkcxP9rUJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4.b Implement your own network\n",
        "\n",
        "For the Oxford Pets dataset, try to write the network by yourself. You can get ideas from existing works, but you cannot directly import them using packages, and the parameter number should be lower than 20M. Train your network from scratch. You would get points if your network can reach an accuracy of 35% (15 pts), and another 15 pts if it reaches 45%. You would want to pay more attention to data augmentation and other hyper-parameters during this part.  Feel free to re-use any functions defined in Part 2."
      ],
      "metadata": {
        "id": "SypYEn509rsU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# example network definition that needs to be modified for custom network stretch goal\n",
        "\n",
        "class Network(nn.Module):\n",
        "    def __init__(self, num_classes=10, dropout = 0.5):\n",
        "        super(Network, self).__init__()\n",
        "        self.features = nn.Sequential(\n",
        "            nn.Conv2d(3, 64, kernel_size=11, stride=4, padding=2),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
        "            nn.Conv2d(64, 256, kernel_size=5, padding=2),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
        "            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
        "        )\n",
        "\n",
        "        self.avgpool = nn.AdaptiveAvgPool2d((6, 6))\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Dropout(p=dropout),\n",
        "            nn.Linear(256 * 6 * 6, 512),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Dropout(p=dropout),\n",
        "            nn.Linear(512, 512),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Linear(512, num_classes),\n",
        "        )\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        N, c, H, W = x.shape\n",
        "        features = self.features(x)\n",
        "        pooled_features = self.avgpool(features)\n",
        "        output = self.classifier(torch.flatten(pooled_features, 1))\n",
        "        return output"
      ],
      "metadata": {
        "id": "gudTxvWmbMlU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "b5SnDgjKVdLJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# from https://gist.github.com/jonathanagustin/b67b97ef12c53a8dec27b343dca4abba\n",
        "# install can take a minute\n",
        "\n",
        "import os\n",
        "# @title Convert Notebook to PDF. Save Notebook to given directory\n",
        "NOTEBOOKS_DIR = \"/content/drive/My Drive/CS441/24SP/hw2\" # @param {type:\"string\"}\n",
        "NOTEBOOK_NAME = \"CS441_SP24_HW2_Solution.ipynb\" # @param {type:\"string\"}\n",
        "#------------------------------------------------------------------------------#\n",
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive/\", force_remount=True)\n",
        "NOTEBOOK_PATH = f\"{NOTEBOOKS_DIR}/{NOTEBOOK_NAME}\"\n",
        "assert os.path.exists(NOTEBOOK_PATH), f\"NOTEBOOK NOT FOUND: {NOTEBOOK_PATH}\"\n",
        "!apt install -y texlive-xetex texlive-fonts-recommended texlive-plain-generic > /dev/null 2>&1\n",
        "!jupyter nbconvert \"$NOTEBOOK_PATH\" --to pdf > /dev/null 2>&1\n",
        "NOTEBOOK_PDF = NOTEBOOK_PATH.rsplit('.', 1)[0] + '.pdf'\n",
        "assert os.path.exists(NOTEBOOK_PDF), f\"ERROR MAKING PDF: {NOTEBOOK_PDF}\"\n",
        "print(f\"PDF CREATED: {NOTEBOOK_PDF}\")"
      ],
      "metadata": {
        "id": "DwOqlBwgEJfa"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    },
    "gpuClass": "standard",
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}