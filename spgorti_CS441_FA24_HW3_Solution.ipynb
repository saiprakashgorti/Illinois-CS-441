{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/saiprakashgorti/Illinois-CS-441/blob/main/spgorti_CS441_FA24_HW3_Solution.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gd7scc-8QJvE"
      },
      "source": [
        "## CS441: Applied ML - HW 3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QagOldZDQJvG"
      },
      "source": [
        "## Part 1: Spam Detection with Naive Bayes Classifier\n",
        "\n",
        "We want to classify text messages as “spam” (unwanted) or “ham” (genuine). We will use data (spam.csv) from the Kaggle SMS spam dataset. We’ve provided the loading and pre-processing code to generate:\n",
        "* `unique_words`: the unique set of words in the dataset\n",
        "* `(x_train, y_train, msg_train)`: counts of words in each message, spam (y=1) or not spam (y=-1) labels, and the message string for each training sample; `x_train[n][j]` is the count of the `j`th word in the `n`th sample.\n",
        "* `*_val` and `*_test`, similar to above, for the val and test splits\n",
        "We will use a Naive Bayes Classifier.\n",
        "\n",
        "See assignment for details (equations are not easy to reproduce here)."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "datadir = \"/content/drive/My Drive/CS441/24FA/hw3/\" # choose proper location\n",
        "\n",
        "# don't change the code below\n",
        "\n",
        "# read data\n",
        "with open(datadir + 'spam.csv', encoding='latin-1') as csvfile:\n",
        "    datareader = csv.reader(csvfile, delimiter=',')\n",
        "    y = np.zeros((10000,))\n",
        "    X = []\n",
        "    n = 0\n",
        "    rownum = 0\n",
        "    for row in datareader:\n",
        "      if rownum == 0:\n",
        "        rownum += 1\n",
        "        continue\n",
        "      rownum += 1\n",
        "      if row[0]=='ham':\n",
        "        y[n] = -1\n",
        "      else:\n",
        "        y[n] = 1\n",
        "      X.append(row[1])\n",
        "      n += 1\n",
        "y= y[:n]\n",
        "\n",
        "# y[n] = -1 for ham, 1 for spam\n",
        "print(y[0])\n",
        "print(X[0])\n",
        "\n",
        "# parse the text messages into words and count the words in each row\n",
        "vectorizer = CountVectorizer(analyzer='word')\n",
        "word_count = vectorizer.fit_transform(X).toarray()\n",
        "\n",
        "print(f\"We have {word_count.shape[0]} examples with {word_count.shape[1]} unique words.\")\n",
        "unique_words = vectorizer.get_feature_names_out()\n",
        "\n",
        "# split data into train (50%), validation (25%), and test (25%)\n",
        "x_train = word_count[::2]\n",
        "y_train = y[::2]\n",
        "msg_train = X[::2]\n",
        "x_val = word_count[1::4]\n",
        "y_val = y[1::4]\n",
        "msg_val = X[1::4]\n",
        "x_test = word_count[3::4]\n",
        "y_test = y[3::4]\n",
        "msg_test = X[3::4]\n"
      ],
      "metadata": {
        "id": "EBzNRkN2AmWA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Train your Naive Bayes Classifier\n",
        "I.e. P(y), P(w|y)) using the train set with =1 and compute the accuracy on the val set.  You should get P(y=1)=0.142, P(call|spam) = 0.0104, and P(call|ham)=0.0029. Your validation accuracy should be higher than 95%."
      ],
      "metadata": {
        "id": "pTZ05VOmAshI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TO DO"
      ],
      "metadata": {
        "id": "nVa3s_iYAwkv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Data exploration\n",
        "What are the 10 spammiest words (i.e. words with highest  logP(wj|spam)- logP(wj|ham))?  What are the 10 hammiest words? Which val message is the spammiest ham (message with highest spam score but y=-1)? Which is hammiest spam (message with lowest spam score but y=1)?  Spammiest spam? Hammiest ham?"
      ],
      "metadata": {
        "id": "me2ooMJ8A1qw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TO DO"
      ],
      "metadata": {
        "id": "i2b_U4qbIa1D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 3. Precision-recall trade-off\n",
        "You want to flag spam messages with minimal false positives. Using the val set, compute precision/recall and display the PR curve. Programmatically, find the threshold with highest recall, where precision > 0.99.  Report the accuracy, precision, and recall on the test set using the same model and the selected threshold."
      ],
      "metadata": {
        "id": "B04qPuOaBIlU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import precision_recall_curve\n",
        "\n",
        "\n",
        "# TO DO"
      ],
      "metadata": {
        "id": "sjbgB-LHBI9y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 2: Robust Estimation\n",
        "\n",
        "The corrupted salary dataset has three variables: salary, years, school.  Salary is the reported salary of each person.  Years is the number of years of experience in the job.  School is the university where the person last had a degree. For the core assignment, we’ll only use salary, and the stretch goals will use the other two variables. Some of the reported salary information is wrong (some incorrect value is provided), so we want to learn things from the data in a way that is robust to the wrong data. We refer to correctly entered data as “valid”.\n",
        "\n",
        "Estimate the true mean, standard deviation, min, and max of the salaries using three different methods."
      ],
      "metadata": {
        "id": "YK769ZQ4NgUP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "datadir = \"/content/drive/My Drive/CS441/24SP/hw3/\"\n",
        "\n",
        "# load data\n",
        "T = np.load(datadir + 'salary.npz')\n",
        "(salary, years, school) = (T['salary'], T['years'], T['school'])"
      ],
      "metadata": {
        "id": "Dzb9-YeTnrie"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Assume no noise\n",
        "Compute the statistics for the data as a whole"
      ],
      "metadata": {
        "id": "zKusZuTmqwIE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TO DO\n",
        "\n",
        "print('Mean: {}  Std: {}  Min: {}   Max: {}'.format(salary_mu, salary_std, salary_min, salary_max)"
      ],
      "metadata": {
        "id": "U8s5LwbpqaR0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Percentiles\n",
        "Assume valid data will fall between the 5th and 95th percentile. Adjust estimates of the min and max by assuming that the valid data has a uniform distribution (see lecture on robust fitting)."
      ],
      "metadata": {
        "id": "8bKArxlbvOvW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pct = 0.05\n",
        "\n",
        "# TO DO\n",
        "\n",
        "print('Mean: {}  Std: {}  Min: {}   Max: {}'.format(salary_mu, salary_std, salary_min, salary_max)"
      ],
      "metadata": {
        "id": "OVXCYlx7wGjS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 3. EM\n",
        "Assume valid data follows a Gaussian distribution, while the fake data has a uniform distribution between the minimum and maximum value of salary. For mean and std, report the estimated mean and std of the valid salary distribution. For min and max, report the min and max salaries that have greater than 50% chance of being valid. Also report the estimated probability that a random sample is valid, and the first five indices of salaries that are not likely to be valid."
      ],
      "metadata": {
        "id": "fdQCUF2aq-9p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# TO DO\n",
        "\n",
        "\n",
        "print('Mean: {}  Std: {}  Min: {}   Max: {}'.format(salary_mu, salary_std, salary_min, salary_max)\n",
        "\n",
        "# print the first five indices of salaries that are not likely to be valid\n",
        "print(np.where(p_valid_given_s<0.5)[0][:5])"
      ],
      "metadata": {
        "id": "lejhsklSqjbL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 3: Stretch Goals\n",
        "Include all your code used for any stretch goals in this section. Add headings where appropriate."
      ],
      "metadata": {
        "id": "3X3j_efPhh6e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TO DO (optional)"
      ],
      "metadata": {
        "id": "UFZfuCqJqhLc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# from https://gist.github.com/jonathanagustin/b67b97ef12c53a8dec27b343dca4abba\n",
        "# install can take a minute\n",
        "\n",
        "import os\n",
        "# @title Convert Notebook to PDF. Save Notebook to given directory\n",
        "NOTEBOOKS_DIR = \"/content/drive/My Drive/CS441/24SP/hw2\" # @param {type:\"string\"}\n",
        "NOTEBOOK_NAME = \"CS441_SP24_HW2_Solution.ipynb\" # @param {type:\"string\"}\n",
        "#------------------------------------------------------------------------------#\n",
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive/\", force_remount=True)\n",
        "NOTEBOOK_PATH = f\"{NOTEBOOKS_DIR}/{NOTEBOOK_NAME}\"\n",
        "assert os.path.exists(NOTEBOOK_PATH), f\"NOTEBOOK NOT FOUND: {NOTEBOOK_PATH}\"\n",
        "!apt install -y texlive-xetex texlive-fonts-recommended texlive-plain-generic > /dev/null 2>&1\n",
        "!jupyter nbconvert \"$NOTEBOOK_PATH\" --to pdf > /dev/null 2>&1\n",
        "NOTEBOOK_PDF = NOTEBOOK_PATH.rsplit('.', 1)[0] + '.pdf'\n",
        "assert os.path.exists(NOTEBOOK_PDF), f\"ERROR MAKING PDF: {NOTEBOOK_PDF}\"\n",
        "print(f\"PDF CREATED: {NOTEBOOK_PDF}\")"
      ],
      "metadata": {
        "id": "5T2IRr7fz6ta"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}